# Docling Intelligence Pipeline

Structured workflow that converts PDFs to Markdown with [docling](https://github.com/DS4SD/docling), counts tokens, and optionally feeds the Markdown into the OpenAI API for downstream intelligence tasks (resumos, insights, planos de a√ß√£o etc.).

## Arquitetura em alto n√≠vel

```
.
‚îú‚îÄ‚îÄ docling_test.py          # CLI orquestrador (converter PDFs, chamar OpenAI ou ambos)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes centrais (pastas, prompts, modelos, chunking)
‚îÇ   ‚îú‚îÄ‚îÄ conversion.py        # Utilit√°rios de convers√£o PDF ‚ûú Markdown
‚îÇ   ‚îú‚îÄ‚îÄ chunking.py          # Planejamento e particionamento do Markdown em chunks
‚îÇ   ‚îî‚îÄ‚îÄ ai_pipeline.py       # Processamento Markdown ‚ûú OpenAI (multi-prompt + chunks)
‚îú‚îÄ‚îÄ pdf_input/               # Coloque aqui os PDFs de origem
‚îú‚îÄ‚îÄ md_output/               # Markdown gerado
‚îú‚îÄ‚îÄ md_output_ia/            # Respostas da IA
‚îú‚îÄ‚îÄ docs/                    # Estrat√©gia e mapas de chunking gerados pelo pipeline
‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias Python
‚îî‚îÄ‚îÄ Rodar_ambiente.txt       # Passo a passo de configura√ß√£o r√°pida
```

## Pr√©-requisitos

- Python 3.10+ (testado com Python 3.13)
- Git (se quiser versionar/publicar)
- Uma chave de API da OpenAI (apenas necess√°ria para a etapa de IA)

## Configura√ß√£o do ambiente (Windows PowerShell)

```powershell
python -m venv .venv
.venv\Scripts\Activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

Depend√™ncias principais:

- `docling`, `safetensors`, `requests`, `certifi` para parsing/OCR de PDFs
- `token_count` para estimar tokens do modelo GPT
- `openai` para conversar com a OpenAI Platform

## Executando o pipeline

Antes de rodar a etapa de IA, copie o modelo de chave e preencha com sua credencial:

```powershell
Copy-Item openai_api_key.txt.example openai_api_key.txt -ErrorAction SilentlyContinue
notepad openai_api_key.txt
```

O arquivo `openai_api_key.txt` est√° no `.gitignore` para evitar commits acidentais. Opcionalmente, voc√™ ainda pode definir a vari√°vel de ambiente `OPENAI_API_KEY` (ou apontar para outro arquivo com `OPENAI_API_KEY_FILE`). Se preferir um `.env`, crie um arquivo com `OPENAI_API_KEY=...`; a aplica√ß√£o carrega automaticamente quando encontrar `python-dotenv` instalado.

Escolha a etapa desejada:

| Etapa             | Comando                                  | Descri√ß√£o                                               |
| ----------------- | ---------------------------------------- | ------------------------------------------------------- |
| Converter         | `python docling_test.py --stage convert` | Converte PDFs em Markdown e registra contagem de tokens |
| IA                | `python docling_test.py --stage ai`      | L√™ Markdown existente e envia para a OpenAI             |
| Completo (padr√£o) | `python docling_test.py`                 | Converte e processa com IA em uma √∫nica execu√ß√£o        |

### Flags √∫teis

- `--dry-run`: apenas lista os arquivos que seriam processados
- `--pdf-dir`, `--md-dir`, `--ai-dir`: sobrescrevem as pastas padr√£o (`md_output_ia` √© o diret√≥rio base da IA)
- `--ai-model`: define outro modelo OpenAI (padr√£o `o4-mini-2025-04-16`)
- `--skip-existing-ai`: preserva respostas j√° geradas (por padr√£o o pipeline sobrescreve)
- `--system-prompt-file`, `--user-prompt-file`: definem prompts a partir de arquivos texto
- `--log-level DEBUG`: habilita logs detalhados

## Configura√ß√µes e prompts

`src/config.py` centraliza as op√ß√µes de runtime:

- **Diret√≥rios**: sobrescreva via env vars (`PDF_INPUT_DIR`, `MD_OUTPUT_DIR`, `AI_OUTPUT_DIR`) ou via CLI
- **Chaves**: `OPENAI_API_KEY` continua v√°lido; alternativamente a aplica√ß√£o l√™ `openai_api_key.txt` (configur√°vel via `OPENAI_API_KEY_FILE`) ou as vari√°veis em `.env`
- **Modelo de tokeniza√ß√£o**: padr√£o `gpt-3.5-turbo` (`TOKEN_COUNTER_MODEL`)
- **Modelo da OpenAI**: padr√£o `o4-mini-2025-04-16` (`OPENAI_MODEL`)
- **Prompts**: edite `prompts/system_prompt.txt` e `prompts/user_prompt.md` para alterar o comportamento padr√£o. Tamb√©m √© poss√≠vel usar env vars (`AI_SYSTEM_PROMPT`, `AI_USER_PROMPT_TEMPLATE`) ou apontar arquivos com `--system-prompt-file` / `--user-prompt-file`.
- **Prompts**: o sistema carrega automaticamente `prompts/system_prompt.txt` e at√© **quatro** arquivos `prompts/user_prompt_partN.md` (N=1..4). Cada parte gera uma chamada independente para a OpenAI, permitindo dividir sa√≠das extensas. Se nenhum arquivo `user_prompt_partN.md` existir, o pipeline usa `prompts/user_prompt.md` como fallback.

> üí° **Como funciona a divis√£o em partes**
>
> - `user_prompt_part1.md` ‚ûú Metadados, datas-chave, valores e pedidos principais.
> - `user_prompt_part2.md` ‚ûú Tipos penais, dados sens√≠veis, lacunas e observa√ß√µes.
> - `user_prompt_part3.md` ‚ûú Linha do tempo e resumo anal√≠tico.
> - `user_prompt_part4.md` ‚ûú Principais eventos detalhados e trechos-fonte.
>
> Cada arquivo mant√©m os placeholders `{document_name}` e `{markdown_content}` para injetar o conte√∫do convertido. Voc√™ pode editar, remover ou adicionar arquivos partindo desse padr√£o. Se preferir um √∫nico prompt, basta excluir/renomear os arquivos em `prompts/user_prompt_part*.md` e manter apenas `user_prompt.md`.

## Chunking do Markdown

Para respeitar limites de contexto dos modelos, o pipeline avalia automaticamente a necessidade de **quebrar o Markdown em chunks** antes de chamar a OpenAI:

- **Modo** (`--chunking` ou env `AI_CHUNKING_MODE`):
  - `auto` (padr√£o) ‚ûú aplica chunking apenas se o documento extrapolar o or√ßamento de contexto estimado
  - `force` ‚ûú sempre fatia o Markdown
  - `off` ‚ûú envia o arquivo inteiro independente do tamanho
- **Tamanho** (`--chunk-target`, `--chunk-max`) ‚ûú define alvo e teto de tokens por chunk; o pipeline usa t√≠tulos e par√°grafos para cortar de forma natural e aplica split extra se necess√°rio.
- **Sobreposi√ß√£o** (`--chunk-overlap`) ‚ûú repete os √∫ltimos N tokens do chunk anterior no in√≠cio do pr√≥ximo para manter continuidade.
- **Estimativa de custo** (`--chunk-price-input`) ‚ûú informa o valor por 1k tokens de entrada para estimar custo em dry-run e salvar no plano.

Cada execu√ß√£o gera um **plano de chunking** (`chunk_metadata/{documento}_chunks.json`) e um **mapa em Markdown** (`chunk_metadata/{documento}_chunk_map.md`) com as se√ß√µes, tokens e intervalos de linhas. Em `--dry-run`, o CLI exibe a quantidade de chunk(s), tokens previstos por requisi√ß√£o e custo estimado.

Quando chunking est√° ativo, os arquivos de sa√≠da seguem o padr√£o:

- `md_output_ia/{documento}_chunkNN_ai_partX.md` ‚ûú resposta da parte X para o chunk NN
- `md_output_ia/{documento}_chunkNN_ai.md` ‚ûú fus√£o das partes daquele chunk
- `md_output_ia/{documento}_ai.md` ‚ûú fus√£o global (todos os chunks, na ordem original)

Se o documento couber em um √∫nico chunk, o pipeline mant√©m os nomes compat√≠veis com a vers√£o anterior (`{documento}_ai_partX.md` e `{documento}_ai.md`).

### Diagn√≥stico p√≥s-processamento

Ao t√©rmino de cada execu√ß√£o (inclusive quando apenas a etapa de IA √© rodada), o CLI imprime um bloco **‚ÄúDiagn√≥stico de Processamento‚Äù** com as seguintes m√©tricas por documento:

1. **Tempo de Docling** e **tamanho do Markdown** ap√≥s a convers√£o.
2. **Resumo de chunking** (quando aplic√°vel): total de chunks, modo utilizado e motiva√ß√£o (ex.: ‚Äúdocumento excede limite dispon√≠vel‚Äù). Cada chunk mostra tokens, palavras, linhas cobertas, tempo/caminho das respostas por prompt e o tamanho do arquivo combinado daquele peda√ßo.
3. **Markdown concatenado global**: tokens e palavras do arquivo final (`{documento}_ai.md`), indicando se foi reutilizado de execu√ß√£o anterior.

Os tempos s√£o mostrados em segundos com duas casas decimais. Se algum est√°gio for pulado (por exemplo, Markdown reaproveitado), o diagn√≥stico sinaliza o motivo.

## Resultados

`docling_test.py` registra a contagem de tokens por documento para estimar custos na OpenAI. As respostas geradas ficam em `md_output_ia/`, onde cada chunk recebe nome pr√≥prio (`{documento}_chunkNN_ai_partX.md` ‚ûú `{documento}_chunkNN_ai.md`) e o arquivo `md_output_ia/{documento}_ai.md` concentra o texto consolidado.

## Publica√ß√£o no GitHub

```powershell
git add .
git commit -m "chore: initial import"
git push -u origin main
```

`.venv/`, `__pycache__/` e `md_output_ia/` j√° est√£o no `.gitignore`.

## Troubleshooting

- **Avisos de symlink do Hugging Face**: suprimidos automaticamente (`HF_HUB_DISABLE_SYMLINKS_WARNING=1`).
- **Erros de SSL ao baixar modelos**: direcione o Requests para o bundle do certifi:
  ```powershell
  $env:REQUESTS_CA_BUNDLE = (python -m certifi)
  ```
- **PDFs grandes**: a primeira execu√ß√£o pode demorar enquanto o docling baixa modelos; execu√ß√µes subsequentes usam cache em `%USERPROFILE%\.cache\huggingface`.
- **Sem API key**: a etapa de IA aborta com mensagem. Configure `OPENAI_API_KEY` e rode novamente.
- **Erro 400 invalid value 'text'**: garanta que o projeto esteja atualizado (`git pull && pip install -r requirements.txt`). A chamada usa o tipo `input_text`, compat√≠vel com a API Responses atual.

## Pr√≥ximos passos sugeridos

- Fragmentar conte√∫dos grandes em blocos menores para respeitar limites de contexto do modelo
- Persistir as respostas tamb√©m em JSON para parsing program√°tico
- Criar testes automatizados (ex.: PDF de exemplo ‚ûú Markdown esperado)
- Adicionar agendamento ou uma interface web leve para disparar o pipeline sob demanda
