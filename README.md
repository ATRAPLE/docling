# Docling Intelligence Pipeline

Structured workflow that converts PDFs to Markdown with [docling](https://github.com/DS4SD/docling), counts tokens, and optionally feeds the Markdown into the OpenAI API for downstream intelligence tasks (resumos, insights, planos de a√ß√£o etc.).

## Arquitetura em alto n√≠vel

```
.
‚îú‚îÄ‚îÄ docling_test.py          # CLI orquestrador (converter PDFs, chamar OpenAI ou ambos)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes centrais (pastas, prompts, modelos)
‚îÇ   ‚îú‚îÄ‚îÄ conversion.py        # Utilit√°rios de convers√£o PDF ‚ûú Markdown
‚îÇ   ‚îî‚îÄ‚îÄ ai_pipeline.py       # Processamento Markdown ‚ûú OpenAI
‚îú‚îÄ‚îÄ pdf_input/               # Coloque aqui os PDFs de origem
‚îú‚îÄ‚îÄ md_output/               # Markdown gerado
‚îú‚îÄ‚îÄ md_output_ia/            # Respostas da IA
‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias Python
‚îî‚îÄ‚îÄ Rodar_ambiente.txt       # Passo a passo de configura√ß√£o r√°pida
```

## Pr√©-requisitos

- Python 3.10+ (testado com Python 3.13)
- Git (se quiser versionar/publicar)
- Uma chave de API da OpenAI (apenas necess√°ria para a etapa de IA)

## Configura√ß√£o do ambiente (Windows PowerShell)

```powershell
python -m venv .venv
.venv\Scripts\Activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

Depend√™ncias principais:

- `docling`, `safetensors`, `requests`, `certifi` para parsing/OCR de PDFs
- `token_count` para estimar tokens do modelo GPT
- `openai` para conversar com a OpenAI Platform

## Executando o pipeline

Antes de rodar a etapa de IA, copie o modelo de chave e preencha com sua credencial:

```powershell
Copy-Item openai_api_key.txt.example openai_api_key.txt -ErrorAction SilentlyContinue
notepad openai_api_key.txt
```

O arquivo `openai_api_key.txt` est√° no `.gitignore` para evitar commits acidentais. Opcionalmente, voc√™ ainda pode definir a vari√°vel de ambiente `OPENAI_API_KEY` (ou apontar para outro arquivo com `OPENAI_API_KEY_FILE`). Se preferir um `.env`, crie um arquivo com `OPENAI_API_KEY=...`; a aplica√ß√£o carrega automaticamente quando encontrar `python-dotenv` instalado.

Escolha a etapa desejada:

| Etapa             | Comando                                  | Descri√ß√£o                                               |
| ----------------- | ---------------------------------------- | ------------------------------------------------------- |
| Converter         | `python docling_test.py --stage convert` | Converte PDFs em Markdown e registra contagem de tokens |
| IA                | `python docling_test.py --stage ai`      | L√™ Markdown existente e envia para a OpenAI             |
| Completo (padr√£o) | `python docling_test.py`                 | Converte e processa com IA em uma √∫nica execu√ß√£o        |

### Flags √∫teis

- `--dry-run`: apenas lista os arquivos que seriam processados
- `--pdf-dir`, `--md-dir`, `--ai-dir`: sobrescrevem as pastas padr√£o (`md_output_ia` √© o diret√≥rio base da IA)
- `--ai-model`: define outro modelo OpenAI (padr√£o `o4-mini-2025-04-16`)
- `--skip-existing-ai`: preserva respostas j√° geradas (por padr√£o o pipeline sobrescreve)
- `--system-prompt-file`, `--user-prompt-file`: definem prompts a partir de arquivos texto
- `--log-level DEBUG`: habilita logs detalhados

## Configura√ß√µes e prompts

`src/config.py` centraliza as op√ß√µes de runtime:

- **Diret√≥rios**: sobrescreva via env vars (`PDF_INPUT_DIR`, `MD_OUTPUT_DIR`, `AI_OUTPUT_DIR`) ou via CLI
- **Chaves**: `OPENAI_API_KEY` continua v√°lido; alternativamente a aplica√ß√£o l√™ `openai_api_key.txt` (configur√°vel via `OPENAI_API_KEY_FILE`) ou as vari√°veis em `.env`
- **Modelo de tokeniza√ß√£o**: padr√£o `gpt-3.5-turbo` (`TOKEN_COUNTER_MODEL`)
- **Modelo da OpenAI**: padr√£o `o4-mini-2025-04-16` (`OPENAI_MODEL`)
- **Prompts**: edite `prompts/system_prompt.txt` e `prompts/user_prompt.md` para alterar o comportamento padr√£o. Tamb√©m √© poss√≠vel usar env vars (`AI_SYSTEM_PROMPT`, `AI_USER_PROMPT_TEMPLATE`) ou apontar arquivos com `--system-prompt-file` / `--user-prompt-file`.
- **Prompts**: o sistema carrega automaticamente `prompts/system_prompt.txt` e at√© **quatro** arquivos `prompts/user_prompt_partN.md` (N=1..4). Cada parte gera uma chamada independente para a OpenAI, permitindo dividir sa√≠das extensas. Se nenhum arquivo `user_prompt_partN.md` existir, o pipeline usa `prompts/user_prompt.md` como fallback.

> üí° **Como funciona a divis√£o em partes**
>
> - `user_prompt_part1.md` ‚ûú Metadados, datas-chave, valores e pedidos principais.
> - `user_prompt_part2.md` ‚ûú Tipos penais, dados sens√≠veis, lacunas e observa√ß√µes.
> - `user_prompt_part3.md` ‚ûú Linha do tempo e resumo anal√≠tico.
> - `user_prompt_part4.md` ‚ûú Principais eventos detalhados e trechos-fonte.
>
> Cada arquivo mant√©m os placeholders `{document_name}` e `{markdown_content}` para injetar o conte√∫do convertido. Voc√™ pode editar, remover ou adicionar arquivos partindo desse padr√£o. Se preferir um √∫nico prompt, basta excluir/renomear os arquivos em `prompts/user_prompt_part*.md` e manter apenas `user_prompt.md`.

As respostas s√£o salvas em `md_output_ia/{documento}_ai_partX.md` (uma para cada parte), **e** o pipeline monta um arquivo consolidado `md_output_ia/{documento}_ai.md` com a concatena√ß√£o das partes na ordem num√©rica. Quando h√° somente um template, apenas o arquivo consolidado √© gerado, mantendo compatibilidade com a vers√£o anterior.

### Diagn√≥stico p√≥s-processamento

Ao t√©rmino de cada execu√ß√£o (inclusive quando apenas a etapa de IA √© rodada), o CLI imprime um bloco **‚ÄúDiagn√≥stico de Processamento‚Äù** com as seguintes m√©tricas por documento:

1. **Tempo de Docling**: dura√ß√£o da convers√£o PDF ‚ûú Markdown.
2. **Tamanho do Markdown**: n√∫mero de tokens (pelo modelo configurado em `TOKEN_COUNTER_MODEL`) e contagem de palavras.
   3‚Äì6. **Tempos dos prompts**: dura√ß√£o individual das respostas da IA para `part1` a `part4` (ou mensagem indicando que o arquivo foi reaproveitado).
3. **Markdown concatenado**: tokens e palavras do arquivo final combinado.

Os tempos s√£o mostrados em segundos com duas casas decimais. Se algum est√°gio for pulado (por exemplo, Markdown reaproveitado), o diagn√≥stico sinaliza o motivo.

## Resultados

`docling_test.py` registra a contagem de tokens por documento para estimar custos na OpenAI. As respostas geradas s√£o salvas em `md_output_ia/{documento}_ai.md`.

## Publica√ß√£o no GitHub

```powershell
git add .
git commit -m "chore: initial import"
git push -u origin main
```

`.venv/`, `__pycache__/` e `md_output_ia/` j√° est√£o no `.gitignore`.

## Troubleshooting

- **Avisos de symlink do Hugging Face**: suprimidos automaticamente (`HF_HUB_DISABLE_SYMLINKS_WARNING=1`).
- **Erros de SSL ao baixar modelos**: direcione o Requests para o bundle do certifi:
  ```powershell
  $env:REQUESTS_CA_BUNDLE = (python -m certifi)
  ```
- **PDFs grandes**: a primeira execu√ß√£o pode demorar enquanto o docling baixa modelos; execu√ß√µes subsequentes usam cache em `%USERPROFILE%\.cache\huggingface`.
- **Sem API key**: a etapa de IA aborta com mensagem. Configure `OPENAI_API_KEY` e rode novamente.
- **Erro 400 invalid value 'text'**: garanta que o projeto esteja atualizado (`git pull && pip install -r requirements.txt`). A chamada usa o tipo `input_text`, compat√≠vel com a API Responses atual.

## Pr√≥ximos passos sugeridos

- Fragmentar conte√∫dos grandes em blocos menores para respeitar limites de contexto do modelo
- Persistir as respostas tamb√©m em JSON para parsing program√°tico
- Criar testes automatizados (ex.: PDF de exemplo ‚ûú Markdown esperado)
- Adicionar agendamento ou uma interface web leve para disparar o pipeline sob demanda
